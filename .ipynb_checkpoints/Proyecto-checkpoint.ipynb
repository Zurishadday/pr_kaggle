{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telstra Network Disruptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Hecho por: Zurisadai Velázquez Manzanero <br>\n",
    "<br>Fecha de inicio: Domingo 5/01/2020<br>\n",
    "<br>Fecha de entrega: Domingo 12/01/2020<br>\n",
    "<br> Visualización desde Github : https://github.com/Zurishadday/pr_kaggle/blob/master/Proyecto.ipynb <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Telstra es una empresa de telecomunicaciones más grande de Australia\n",
    "\n",
    "Objetivo: Telstra desea predecir la gravedad de las interrupciones en su red con un conjunto de datos (propocionados en kaggle). Su fin último es mejorar la experiencia del cliente prediciendo con anticipación las futuras interrupciones en ubicaciones específicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Archivos**\n",
    "- train.csv - el conjunto de entrenamiento para la severidad de la falla\n",
    "- test.csv - el conjunto de prueba para la severidad de la falla\n",
    "- sample_submission.csv – una muestra del formato correcto para la entrada\n",
    "- event_type.csv: tipo de evento relacionado con el conjunto de datos principal\n",
    "- log_feature.csv - características extraídas de los archivos de registro\n",
    "- resource_type.csv: tipo de recurso relacionado con el conjunto de datos principal\n",
    "- severity_type.csv: tipo de severidad de un mensaje de advertencia que proviene del registro\n",
    "\n",
    "\n",
    "- **fault_severity** (target) = La gravedad de la falla  y tiene 3 categorías: 0,1,2 (0 significa sin falla, 1 significa solo unas pocas y 2 significa muchas). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfevent = pd.read_csv(r\"~\\Downloads\\telstra_recruiting_network\\event_type.csv\")\n",
    "dflog = pd.read_csv(r\"~\\Downloads\\telstra_recruiting_network\\log_feature.csv\")\n",
    "dfresource = pd.read_csv(r\"~\\Downloads\\telstra_recruiting_network\\resource_type.csv\")\n",
    "dfsubmission = pd.read_csv(r\"~\\Downloads\\telstra_recruiting_network\\sample_submission.csv\")\n",
    "dfseverity = pd.read_csv(r\"~\\Downloads\\telstra_recruiting_network\\severity_type.csv\")\n",
    "dftest = pd.read_csv(r\"~\\Downloads\\telstra_recruiting_network\\test.csv\")\n",
    "dftrain = pd.read_csv(r\"~\\Downloads\\telstra_recruiting_network\\train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptivo del archivo  \"event_type.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De los event_types existentes, 8015 clientes tienen un solo event_type, 8873 tienen dos event_type y así sucesivamente hasta que un cliente tiene 11 event_type diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id\n",
       "event_type      \n",
       "1           8015\n",
       "2           8873\n",
       "3           1375\n",
       "4            222\n",
       "5             34"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg = dfevent.groupby('id').aggregate('count').reset_index()\n",
    "dg.groupby('event_type').aggregate('count').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo que se hará a continuación es ver que event_types son más relevantes dado su repetición. Hay que recordar que existen en la base de dfevent: 31170 registros. Y nosotros por simplicidad y relevancia estadística se tomarán los event_type que más se repiten por el dilema de Pareto. \n",
    "- event_type: 11,35,34,15 y 20. Los anteriores event_type conforman alrededor del 84% de los datos. Por relevancia solo incluiremos esos únicamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>event_type 11</th>\n",
       "      <td>7888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_type 35</th>\n",
       "      <td>6615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_type 34</th>\n",
       "      <td>5927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_type 15</th>\n",
       "      <td>4395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_type 20</th>\n",
       "      <td>1458</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id\n",
       "event_type         \n",
       "event_type 11  7888\n",
       "event_type 35  6615\n",
       "event_type 34  5927\n",
       "event_type 15  4395\n",
       "event_type 20  1458"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfevent.groupby('event_type').aggregate('count').sort_values(by=['id'],ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptivo del archivo \"resource_type.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De igual forma se observan que los Clientes llegan a tener al mismo tiempo 5 diferentes resource_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resource_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id\n",
       "resource_type       \n",
       "1              16421\n",
       "2               1814\n",
       "3                257\n",
       "4                 44\n",
       "5                 16"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg = dfresource.groupby('id').aggregate('count').reset_index()\n",
    "dg.groupby('resource_type').aggregate('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por relevancia incluiremos el resource_type 8 y 2 que conforman el 91% de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resource_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>resource_type 8</th>\n",
       "      <td>10268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resource_type 2</th>\n",
       "      <td>8918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resource_type 6</th>\n",
       "      <td>582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resource_type 7</th>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resource_type 4</th>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resource_type 9</th>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resource_type 3</th>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resource_type 10</th>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resource_type 1</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resource_type 5</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id\n",
       "resource_type          \n",
       "resource_type 8   10268\n",
       "resource_type 2    8918\n",
       "resource_type 6     582\n",
       "resource_type 7     498\n",
       "resource_type 4     330\n",
       "resource_type 9     190\n",
       "resource_type 3     145\n",
       "resource_type 10     73\n",
       "resource_type 1      58\n",
       "resource_type 5      14"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfresource.groupby('resource_type').aggregate('count').sort_values(by=['id'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptivo del archivo \"log_feature.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El caso de este archivo es especial. El siguiente código muestra que cada 'feature #' tienen diferentes volúmenes. Se podrían agrupar sacando la media, pero en este caso no es posible hacerlo, ya que los volúmenes son características del feature que lo vuelven único.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "log_feature  volume\n",
       "feature 1    1          20\n",
       "             2           1\n",
       "feature 10   1          11\n",
       "             2           2\n",
       "             3           5\n",
       "             4           1\n",
       "             5           2\n",
       "             6           1\n",
       "             7           1\n",
       "             14          1\n",
       "feature 100  2           1\n",
       "feature 101  6           3\n",
       "             7           2\n",
       "             30          1\n",
       "             36          1\n",
       "             41          1\n",
       "             43          1\n",
       "             48          1\n",
       "             53          1\n",
       "             57          1\n",
       "             59          1\n",
       "             69          1\n",
       "             73          1\n",
       "             91          1\n",
       "             92          1\n",
       "             93          1\n",
       "             94          1\n",
       "             106         1\n",
       "             123         1\n",
       "             134         1\n",
       "                      ... \n",
       "feature 87   10          2\n",
       "             11          2\n",
       "             15          1\n",
       "             18          1\n",
       "             37          1\n",
       "feature 88   1           2\n",
       "feature 89   1           2\n",
       "feature 9    4           1\n",
       "feature 90   1           2\n",
       "feature 91   1           2\n",
       "feature 92   1           2\n",
       "feature 93   1           2\n",
       "feature 94   1         109\n",
       "             2          20\n",
       "             3           4\n",
       "             4           3\n",
       "             5           1\n",
       "             6           1\n",
       "             9           1\n",
       "             15          1\n",
       "feature 95   1          43\n",
       "             2           7\n",
       "             3           1\n",
       "feature 96   5           1\n",
       "feature 97   1           2\n",
       "feature 98   1           3\n",
       "             2           3\n",
       "             3           1\n",
       "             5           1\n",
       "feature 99   1           3\n",
       "Length: 3972, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dflog.groupby(['log_feature', 'volume']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La mejor forma de proceder es extraer los principales features y luego también extraer los principales volúmenes por feature.\n",
    "\n",
    "La siguiente línea de código muestra que los principales feature son los siguientes:'feature 312','feature 232','feature 82','feature 203','feature 313','feature 233','feature 307','feature 54','feature 170','feature 71','feature 315','feature 134','feature 80','feature 235','feature 193','feature 219','feature 68'\n",
    "\n",
    "Y del volumen son:  1 al 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature 312    5267\n",
       "feature 232    4754\n",
       "feature 82     3472\n",
       "feature 203    2823\n",
       "feature 313    2145\n",
       "Name: log_feature, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dflog.log_feature.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    20713\n",
       "2     9716\n",
       "3     4488\n",
       "4     3713\n",
       "5     2207\n",
       "Name: volume, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dflog.volume.value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptivo del archivo \"severity_type.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, severity se toman todos los campos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "severity_type 2    8737\n",
       "severity_type 1    8728\n",
       "severity_type 4    1014\n",
       "severity_type 5      65\n",
       "severity_type 3       8\n",
       "Name: severity_type, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfseverity.severity_type.value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez entendido los datos, se toman las principales variables por relevancia y se meten en la función \"get_dummies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>severity_typeseverity_type 1</th>\n",
       "      <th>severity_typeseverity_type 2</th>\n",
       "      <th>severity_typeseverity_type 3</th>\n",
       "      <th>severity_typeseverity_type 4</th>\n",
       "      <th>severity_typeseverity_type 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  severity_typeseverity_type 1  severity_typeseverity_type 2  \\\n",
       "0   1                             1                             0   \n",
       "1   2                             0                             1   \n",
       "2   3                             1                             0   \n",
       "3   4                             0                             0   \n",
       "4   5                             0                             1   \n",
       "\n",
       "   severity_typeseverity_type 3  severity_typeseverity_type 4  \\\n",
       "0                             0                             0   \n",
       "1                             0                             0   \n",
       "2                             0                             0   \n",
       "3                             0                             1   \n",
       "4                             0                             0   \n",
       "\n",
       "   severity_typeseverity_type 5  \n",
       "0                             0  \n",
       "1                             0  \n",
       "2                             0  \n",
       "3                             0  \n",
       "4                             0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfseverity=pd.get_dummies(dfseverity,prefix_sep='')\n",
    "dfseverity=dfseverity.groupby(['id']).aggregate('sum').reset_index()\n",
    "dfseverity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>resource_typeresource_type 2</th>\n",
       "      <th>resource_typeresource_type 8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  resource_typeresource_type 2  resource_typeresource_type 8\n",
       "0   1                             0                             1\n",
       "1   2                             1                             0\n",
       "2   3                             0                             1\n",
       "3   4                             1                             0\n",
       "4   5                             1                             0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "principal=['resource_type 8','resource_type 2']\n",
    "dfresource=dfresource.loc[dfresource.resource_type.isin(principal),:]\n",
    "dfresource=pd.get_dummies(dfresource,prefix_sep='')\n",
    "dfresource=dfresource.groupby(['id']).aggregate('sum').reset_index()\n",
    "dfresource.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>event_typeevent_type 11</th>\n",
       "      <th>event_typeevent_type 15</th>\n",
       "      <th>event_typeevent_type 20</th>\n",
       "      <th>event_typeevent_type 34</th>\n",
       "      <th>event_typeevent_type 35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  event_typeevent_type 11  event_typeevent_type 15  \\\n",
       "0   1                        1                        0   \n",
       "1   2                        0                        0   \n",
       "2   3                        1                        0   \n",
       "3   5                        0                        0   \n",
       "4   6                        0                        0   \n",
       "\n",
       "   event_typeevent_type 20  event_typeevent_type 34  event_typeevent_type 35  \n",
       "0                        0                        0                        0  \n",
       "1                        0                        1                        1  \n",
       "2                        0                        0                        0  \n",
       "3                        0                        1                        1  \n",
       "4                        0                        1                        0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "principal=['event_type 11','event_type 35','event_type 34','event_type 15','event_type 20']\n",
    "dfevent=dfevent.loc[dfevent.event_type.isin(principal),:]\n",
    "dfevent=pd.get_dummies(dfevent,prefix_sep='')\n",
    "dfevent=dfevent.groupby(['id']).aggregate('sum').reset_index()\n",
    "dfevent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "principal=['feature 312','feature 232','feature 82','feature 203','feature 313','feature 233','feature 307','feature 54'\n",
    "          ,'feature 170','feature 71','feature 315','feature 134','feature 80','feature 235','feature 193','feature 219','feature 68']\n",
    "dflog=dflog.loc[dflog.log_feature.isin(principal),:]\n",
    "dflog=dflog.loc[dflog.volume<7,:]\n",
    "dflog.set_index(['id'],inplace=True)\n",
    "dflogX=pd.get_dummies(\n",
    "                    pd.DataFrame(\n",
    "                            dflog[['log_feature', 'volume']].apply(lambda x: '_'.join(x.astype(str)), axis=1)\n",
    "                    ,columns=[''],index=dflog.index)\n",
    "            ,prefix_sep='').reset_index()\n",
    "dflogX=dflogX.groupby(['id']).aggregate('sum').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins de los archivos con el archivo \"train.csv\" que contiene la variable objetivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente código muestra una forma reducida de realizar el join y posteriormente rellenar los missing values por ceros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = [dftrain,dfevent, dflogX, dfresource,dfseverity]\n",
    "lf = reduce(lambda left, right: pd.merge(left,right,on='id',how='left'), df)\n",
    "lf.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También se realizó el anterior código con python desde Dataiku y su respectivo Join."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/jwxSVt3/join-dataiq.png\" alt=\"join-dataiq\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = t.drop(columns=['fault_severity'])\n",
    "#y = t['fault_severity']\n",
    "\n",
    "X = lf.drop(columns=['fault_severity','id','location'])\n",
    "y = lf['fault_severity']\n",
    "\n",
    "# tomo una muestra estratificada del 20%\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42,test_size=0.20,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train,columns=list(X.columns))\n",
    "X_test  = pd.DataFrame(X_test,columns=list(X.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploramos el número de muestras que tenemos por cada categoría de la variable objetivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fault_severity</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id\n",
       "fault_severity      \n",
       "0               4784\n",
       "1               1871\n",
       "2                726"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lf.groupby('fault_severity').count()[['id']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde Dataiku\n",
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/30djrRj/Ydatai1.png\" alt=\"Ydatai1\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que para balancear el dataset podemos darle un peso a cada categoría de manera que queden parejas:\n",
    "    \n",
    "faul_severity '0': 4784/4784= 1 <br>\n",
    "faul_severity '1':4781/1871 = 2.55 <br>\n",
    "faul_severity '2':4781/726 = 6.58 <br>\n",
    "\n",
    "\n",
    "C0 = 1 <br>\n",
    "C1 = 2.55 <br>\n",
    "C2 = 6.58 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seteamos los parámetros para varios modelos: <br>\n",
    "en el caso de class_weight pondremos los coeficientes que encontramos y unos mas conservadores en el peso de clase\n",
    "\n",
    "Parámetros de la Logit:\n",
    "- solver : Algortimo utilizado para minimizar la loss-function\n",
    "- 'multi_class': se elige 'multinomial' por ser un problema con target multicategórico\n",
    "- class_weight: Se dan los pesos que se les quiere otorgar a las clases [{0: 1, 1: 2.55,2:6.58},{0: 1, 1: 1.5,2:3.5}]\n",
    "\n",
    "Parámetros del XGBoost:\n",
    "- 'objective': se elige 'multi:softmax' por ser un problema multicategórico\n",
    "- 'max_depth': máximo número de descendencias de los árboles\n",
    "- 'eval_metric': métrica de evaluación, log-loss multicategórica\n",
    "- 'C': Parámetro de regularización para outliers\n",
    "- Class weigth: seteado igual que antes\n",
    "\n",
    "Parámetros del SVC:\n",
    "- 'Gamma': Parámetro de influencia radial de los puntos \n",
    "- 'C': Parámetro de regularización.\n",
    "\n",
    "Parámetros del RF:\n",
    "- 'n_estimators' : número de arboles\n",
    "- 'criterio' : criterio de corte\n",
    "- max features: máximo número de variables por árbol\n",
    "- max depth: máximo número de descendencias para prevenir overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramsLogit   = {'solver':['newton-cg', 'lbfgs','sag', 'saga'],'multi_class':['multinomial'],'class_weight':[{0: 1, 1: 2.55,2:6.58},{0: 1, 1: 1.5,2:3.5}]}\n",
    "paramsXGBoost = {'objective':['multi:softmax'],'max_depth':[4,5,6,7],'eval_metric' : ['mlogloss'],'C':[0.1,.5,1,5],'class_weight':[{0: 1, 1: 2.55,2:6.58},{0: 1, 1: 1.5,2:3.5}]}\n",
    "paramsRF      = {'n_estimators':[60,80,100,120],'criterion':['gini','entropy'],'max_features':['sqrt','log2'],'max_depth':[5,6,7],'class_weight':[{0: 1, 1: 2.55,2:6.58},{0: 1, 1: 1.5,2:3.5}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- log_loss: cuanto menor sea la puntuación, mejor será el modelo.\n",
    "\n",
    "- neg_log_loss:la puntuación es negativa: cuanto mayor sea la puntuación, mejor será el modelo.\n",
    "\n",
    " -cv: Quiere decir Cross-Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Logit   = LogisticRegression()\n",
    "XGBoost = XGBClassifier()\n",
    "RFC     = RandomForestClassifier()\n",
    "\n",
    "GS_Logit   = GridSearchCV(Logit,   n_jobs=-1, cv=5,  scoring='neg_log_loss', param_grid=paramsLogit,verbose=3)\n",
    "GS_XGBoost = GridSearchCV(XGBoost, n_jobs=-1, cv=5,  scoring='neg_log_loss', param_grid=paramsXGBoost,verbose=3)\n",
    "GS_RFC     = GridSearchCV(RFC,     n_jobs=-1, cv=5,  scoring='neg_log_loss', param_grid=paramsRF,verbose=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza una Estandarización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   29.8s\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:   38.2s finished\n",
      "D:\\Users\\zvelazquez\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=-1)]: Done 160 out of 160 | elapsed: 12.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:    8.9s\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:   21.9s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   43.9s\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:  1.2min finished\n"
     ]
    }
   ],
   "source": [
    "GS_Logit.fit(X_train_s,y_train)\n",
    "GS_XGBoost.fit(X_train,y_train)\n",
    "GS_RFC.fit(X_train,y_train)\n",
    "\n",
    "Best_Logit = GS_Logit.best_estimator_\n",
    "Best_XGBoost = GS_XGBoost.best_estimator_\n",
    "Best_RFC = GS_RFC.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\zvelazquez\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight={0: 1, 1: 1.5, 2: 3.5},\n",
       "            criterion='gini', max_depth=7, max_features='sqrt',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=80, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Best_Logit.fit(X_train_s,y_train)\n",
    "Best_XGBoost.fit(X_train,y_train)\n",
    "Best_RFC.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza la predicción del mejor modelo encontrado de acuerdo al parametro de neg_log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\zvelazquez\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "y_pred_Logit = Best_Logit.predict(X_test_s)\n",
    "y_pred_XGBoost = Best_XGBoost.predict(X_test)\n",
    "y_pred_RFC = Best_RFC.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob_Logit = Best_Logit.predict_proba(X_test_s)\n",
    "y_prob_XGBoost = Best_XGBoost.predict_proba(X_test)\n",
    "y_prob_RFC = Best_RFC.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se obtiene que el mejor modelo es  **XGBoost con un 0.69465 de accuricy y un 0.666 de Log loss.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Logit': 0.6452268111035884, 'XGBoost': 0.6946513202437373, 'RFC': 0.6479350033852403}\n"
     ]
    }
   ],
   "source": [
    "accur_Logit = accuracy_score(y_test,y_pred_Logit)\n",
    "accur_XGBoost = accuracy_score(y_test,y_pred_XGBoost)\n",
    "accur_RFC = accuracy_score(y_test,y_pred_RFC)\n",
    "\n",
    "dic_accur = {'Logit':accur_Logit,'XGBoost':accur_XGBoost,'RFC':accur_RFC}\n",
    "\n",
    "print(dic_accur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6665892149577107"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#log los XGBoost\n",
    "log_loss(y_test,y_prob_XGBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por parte de la herramienta Dataiku, se obtuvo los siguientes resultados con los parametros por Default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://ibb.co/v3VtKDk\"><img src=\"https://i.ibb.co/PcFSftg/dataig.png\" alt=\"dataig\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[729,  96, 132],\n",
       "       [193, 127,  55],\n",
       "       [ 37,  11,  97]], dtype=int64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_pred_Logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[898,  43,  16],\n",
       "       [258, 103,  14],\n",
       "       [107,  13,  25]], dtype=int64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_pred_XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[767,  45, 145],\n",
       "       [221,  90,  64],\n",
       "       [ 39,   6, 100]], dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_pred_RFC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conclusiones:\n",
    "<br> Existen veces que de acuerdo a la función de mejor desempeño (en este caso log loss) no es el único KPI a considerar para poner en productivo un modelo. Es importante también evaluar cual modelo se generaría menos perdida monetaria o de insatisfacción por parte de la unidad de negocio como en este caso el cliente que es Telstra. Por lo que existen otros indicares también podrían ser la estabilidad del modelo a través del tiempo o la interpretabilidad. <br>\n",
    "\n",
    "<br>También una cosa a resaltar cual de los modelos prefieres dado la menor cantidad de Falsos Positivos o Falsos Negativos según el modelo. Normalmente se defino eso con la unidad de Negocio.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se crea el archivo para el testing y se manda a Kaggle para el Final Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11171 entries, 0 to 11170\n",
      "Data columns (total 2 columns):\n",
      "id          11171 non-null int64\n",
      "location    11171 non-null object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 174.6+ KB\n"
     ]
    }
   ],
   "source": [
    "dftest.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = [dftest,dfevent, dflogX, dfresource,dfseverity]\n",
    "lf1 = reduce(lambda left, right: pd.merge(left,right,on='id',how='left'), df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf1.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lf1.drop(columns=['id','location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\zvelazquez\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "y_prob_XGBoost = Best_XGBoost.predict_proba(X)\n",
    "y_pred_XGBoost = Best_XGBoost.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df=pd.DataFrame(Best_XGBoost.predict_proba(X),columns=['predict_0', 'predict_1', 'predict_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission=pd.concat([lf1[['id']],pred_df],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea el archivo pred2 para el scoring de Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[['id','predict_0','predict_1','predict_2']].to_csv(r\"~\\Downloads\\pred2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/x7my7pq/kaggle.png\" alt=\"kaggle\" border=\"0\"></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
